{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Simultaneous Localization and Mapping\n",
    "\n",
    "Simultaneous Localization And Mapping (SLAM) is one of the most fundamental problems in robotics.\n",
    "It arises when our robot doesn't have access to a map of the environment nor the pose where it is located.\n",
    "In this way, the SLAM problem is more complex than the two separate ones in the previous chapters, localization and mapping.\n",
    "\n",
    "There are mainly two ways to address the SLAM problem: **Full SLAM** and **Online SLAM**.\n",
    "\n",
    "## 7.1 Full SLAM\n",
    "\n",
    "Estimates the whole path traversed at each step, that is: \n",
    "\n",
    "$$p( x_{1:k}, m_{1:L} | z_{1:k}, u_{1:k})$$ \n",
    "\n",
    "This issue can be faced by means of the GraphSLAM technique, where landmarks and robot poses are represented as nodes in a graph linked by arcs stating odometry and/or observations. Its general idea is that arcs are constraints for the free movement of the nodes. $\\\\[10pt]$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/graphslam.png\"/>\n",
    "    <figcaption>Fig 1. Example of SLAM with GraphSLAM.</figcaption>\n",
    "</center>\n",
    "\n",
    "A simplified version of this technique, Pose GraphSLAM, is typically prefered. In this case, nodes are unknown robot poses, and arcs represent odometry information or common observed landmarks from different poses. $\\\\[10pt]$\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/pose_graphslam.png\"/>\n",
    "    <figcaption>Fig 2. Example of SLAM with Pose GraphSLAM.</figcaption>\n",
    "</center>\n",
    "\n",
    "\n",
    "## 7.2 Online SLAM\n",
    "\n",
    "Which only estimates the latest pose (we do not consider $x_{1:k-1}$), so:\n",
    "\n",
    "$$p(x_{k}, m_{1:L} | z_{1:k}, u_{1:k})$$\n",
    "\n",
    "A popular approach for dealing with this problem is the Extended Kalman Filter (EKF).\n",
    "\n",
    "## 7.3 Additional remarks\n",
    "\n",
    "As in the previous chapters, we assume here that **data association** is given, that is, we know which feature is being seen by each sensor observation!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\">OPTIONAL</span>\n",
    "\n",
    "<span style=\"color:green\">Surf the internet looking for more general information about SLAM. You can include additional definitions, examples, images, videos,... anything you find interesting!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your work here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">***END OF OPTIONAL PART***</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
